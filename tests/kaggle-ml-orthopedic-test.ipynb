{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import test train split, cross_val_score, and GridSearchCV\n",
    "\n",
    "# Import classification_report, confusion_matrix, and roc_curve\n",
    "\n",
    "# Import the standard scaler \n",
    "\n",
    "# Import Pipeline and make_pipeline\n",
    "\n",
    "# Import the k nearest neighbors classifier.\n",
    "# Import kmeans clustering\n",
    "# Import a random forest classifier\n",
    "# Import linear regression \n",
    "# Import logistic regression\n",
    "# Import SVC\n",
    "# Import PCA\n",
    "# Import t-sne (from manifold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/biomechanical-features-of-orthopedic-patients/column_2C_weka.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the basic EDA: look at the columns, the info, the head, and the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some plotting. Using a list comprehension make a list `color_list` which assigns red to abnormal entries and green to normal entries in `data['class']`. \n",
    "\n",
    "HINT: I keep messing up the indexing here, don't forget to use `.loc`\n",
    "\n",
    "Then, use pandas plotting functionality to make a scatter matrix of all the features NOT in `data['class']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Also use the following additional \n",
    "# arguments:\n",
    "\n",
    "# pd., #<-- start the scatter matrix here, with plotting and scatter_matrix calls.\n",
    "#                              c = color_list,\n",
    "#                              figsize = [15,15],\n",
    "#                              diagonal = 'hist',\n",
    "#                              alpha=.5,\n",
    "#                              s = 200,\n",
    "#                              marker = '*',\n",
    "#                              edgecolor = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build one of the simplest possible predictive models. \n",
    "There are a number of parts to this. \n",
    "\n",
    "STEP One: make two variables X and y. X is to be everything except the Abnormal/Normal class and y is to be class. \n",
    "\n",
    "STEP Two: With X and y in place, make the split, with test size being thirty percent of the total and a random state of 1 \n",
    "\n",
    "STEP Three: Instantiate the KNeighborsClassifier object with k = 3\n",
    "\n",
    "STEP Four: fit the new object on X_train and y_train, print the score on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But, we have many options for the size of k. Fill in the blanks in the \n",
    "# following code to make a series of different KNN models with varying\n",
    "# sizes of k, testing to see which is the best. \n",
    "\n",
    "# neigh = np.______(1,25)\n",
    "# train_accuracy = []\n",
    "# test_accuracy = []\n",
    "\n",
    "# for k in neigh:\n",
    "#     knn = KNeighborsClassifier(__________ = k)\n",
    "#     knn.fit(X_train, y_train)\n",
    "#     train_accuracy.append(knn.score(X_train, y_train))\n",
    "#     test_accuracy.append(knn.score(X_test, y_test))\n",
    "    \n",
    "# plt.plot(figsize = [13,8])\n",
    "# plt.plot(neigh, test_accuracy, label = 'Testing Accuracy')\n",
    "# plt.plot(neigh, train_accuracy, label = 'Training Accuracy')\n",
    "# plt.legend()\n",
    "# plt.title('-value VS Accuracy')\n",
    "# plt.xlabel('Number of Neighbors')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xticks(neigh)\n",
    "# plt.show()\n",
    "\n",
    "# print(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),\n",
    "#                                         1+test_accuracy.index(np.max(test_accuracy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "# We can't work with both normal and abnormal classes at the same time\n",
    "# when building a linear regression, so begin by making a variable \n",
    "# 'data_ab' which contains just the 'abnormal' values in 'data'.\n",
    "# HINT: there's no use of .loc here, we're just indexing into the dataframe wherever \n",
    "# class is abnormal. \n",
    "\n",
    "\n",
    "# Then, make an np array out of 'x1' = 'pelvic_incidence' and 'y1' = 'sacral_slope'\n",
    "# from this data_ab. Reshape them so that their column dimension is \n",
    "# 1. \n",
    "\n",
    "# Linear regression won't work at all if the data aren't, y'know, \n",
    "# linear, so let's do some scatter plotting first to ensure that they\n",
    "# are. We'll need plt.figure() with a figsize of 10,10, plt.scatter(),\n",
    "# plt.xlabel(), plt.ylabel(), and plt.show() commands, with some arguments\n",
    "# passed in for all but the last. \n",
    "\n",
    "# Instantiate the linear regression in the variable 'reg'. \n",
    "\n",
    "\n",
    "# Make a variable 'predict_space' with .linspace(), its dimensions equal\n",
    "# to the min of x1 and the max of x1, reshaped to preserve row dimensions\n",
    "# and to force column dimensions to be equal to 1. \n",
    "# I DON'T KNOW WHY THIS IS NECESSARY. \n",
    "\n",
    "# Now fit reg on x1 and y1. Then make a variable 'predicted' with \n",
    "# reg's predict method, passing it predict_space. Use .score to return \n",
    "# the R2 score for x1 and y1.\n",
    "\n",
    "# use plt.plot() with predict_space, predicted, color='black', and linewidth=.8 as arguments, \n",
    "# plt.scatter(), plt.xlabel(), plt.ylabel(), and plt.show() to plot the actual line through \n",
    "# the data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for some cross validation. Import the cross validation score from \n",
    "# the right sub-branch of sklearn. \n",
    "\n",
    "k = 5 # This is the number of 'folds'.\n",
    "\n",
    "# Make a variable 'cv_result' equal to the cross validation score taking \n",
    "# the linear regression from above, x1, y1, and cv = k as arguments. Print\n",
    "# the cv score (no .score() required here) and the cv score averages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge and Lasso regression skipped until I can figure out why they\n",
    "# keep throwing errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for a confusion matrix and a random forest classifier. \n",
    "\n",
    "# Import classification report and confusion matrix from sklearn.metrics,\n",
    "# and import random forest from the appropriate sub-branch. \n",
    "\n",
    "\n",
    "# Make new x,y vars as everything except 'class' and 'class', respectively. I don't\n",
    "# know if this is required but it's in the original. \n",
    "\n",
    "\n",
    "\n",
    "# Now make a train test split with x and y.\n",
    "\n",
    "\n",
    "# instantiate a Random Forest and call it rf, pass in random_state = 4 as \n",
    "# an argument. Fit it in X_train and y_train. Make a variable 'y_pred' and \n",
    "# by calling rf's predict method on X_test. \n",
    "\n",
    "\n",
    "# Instantiate a confusion matrix 'cm', pass it y_test and y_pred. Print it, \n",
    "# then print the classification report  as well (classification report requires two arguments,\n",
    "# y_test and y_pred. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression and ROC curve \n",
    "\n",
    "# Import roc_curve from sklearn.metrics, then logistic regression.\n",
    "\n",
    "\n",
    "# We need to make a 'binary_class' column in data. Do this with a listcomp,\n",
    "# giving 1 to 'Abnormal' and 0 to 'Normal'. \n",
    "\n",
    "\n",
    "# Then re-make x = everything that is NOT 'class' AND NOT 'class_binary' \n",
    "# (this is different from before, we'll need that conjunction) and y = \n",
    "# equal to 'class_binary'. \n",
    "\n",
    "# Make another test/train split, test size 30% and random state 42 \n",
    "\n",
    "# instantiate the logistic regression, then fit on X_train and y_train.\n",
    "# Make a variable 'y_pred_proba' with logistic regressions predict_proba \n",
    "# method passed x_test as an argument(use [:, 1] to index the correct value\n",
    "# (I don't know why.)\n",
    "\n",
    "\n",
    "\n",
    "# Populate the fpr, tpr, and threshold variables by passing y_test and \n",
    "# y_pred_proba to roc_curve.\n",
    "\n",
    "\n",
    "# Now for plotting:\n",
    "\n",
    "# plt.plot([0,1],[0,1], 'k--')\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search on KNN \n",
    "\n",
    "\n",
    "# Make a variable 'grid', a dictionary with a single key 'n_neighbors' \n",
    "# equal to an np array with values from 1 to 50. \n",
    "\n",
    "\n",
    "# Instantiate a K Nearest Neighbor called 'knn'.\n",
    "\n",
    "\n",
    "# Make a 'knn_cv' equal to GridSearchCV, with 'knn', 'grid', and cv=3 \n",
    "# passed in as arguments. Then use it to fit on x and y. \n",
    "\n",
    "\n",
    "# Print out the best parameters and best score from knn_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on logistic regression. \n",
    "\n",
    "# We need another parameter grid. As before the two keys will be strings,\n",
    "# 'C' and 'Penalty'. The value to the first is np.logspace(-3,3,7), the \n",
    "# value of the second is a list of the l1 and l2 penalties as strings.\n",
    "\n",
    "# Use this, no need to reiterate:\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .3, random_state = 12)\n",
    "\n",
    "# Instantiate a logistic regression 'logreg', make a var 'logreg_cv' as \n",
    "# an instantiation of GridSearchCV(), with logreg, param_grid, and cv = 3)\n",
    "# passed in as arguments. Fit it on X_train, y_train. \n",
    "\n",
    "\n",
    "# Print out the best parameters and best score from logreg_cv. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing. \n",
    "\n",
    "\n",
    "# For pipeline to work we need to make a variable 'steps' which consists\n",
    "# of a set of tuples detailing what to do. The first tuple will be 'scaler'\n",
    "# and an instatiation of same, and the second will be 'SVM' and an \n",
    "# instantiation of SVC. \n",
    "\n",
    "# Make a variable 'pipeline' which is an instantiation of Pipeline with 'steps\n",
    "# passed in. \n",
    "\n",
    "# We also need the following parameters variable:\n",
    "\n",
    "# parameters = {'SVM__C': [1,10,100],\n",
    "#               'SVM__gamma': [.1, .01]}\n",
    "\n",
    "# Use this train test split:\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = .2, random_state = 1)\n",
    "\n",
    "# Now, make 'cv' as an instantiation of GridSearchCV with pipeline, \n",
    "# param_grid = parameters, cv = 3. Fit it on X_train, and y_train. \n",
    "\n",
    "\n",
    "# Make some predictions with cv on X_test, store them in 'y_pred'. \n",
    "\n",
    "# Print cv's score on X_test and y_test, as well as its best parameters. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED LEARNING \n",
    "\n",
    "# To simulate this we'll make a var 'data2' and put just 'pelvic_radius' and \n",
    "# 'degree_spondylolisthesis' in it. \n",
    "\n",
    "# Instantiate KMeans with two clusters, then fit it on data2. Use it to \n",
    "# make some predictions which we'll store in labels. \n",
    "\n",
    "# Now scatter plot data['pelvic_radius'], data['degree_spondylolisthesis'],\n",
    "# and make c = labels. \n",
    "           \n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does all this clustering mean? Let's do a cross tabulation table. \n",
    "\n",
    "# Make 'df' equal to DataFrame with a dictionary containing the strings'labels' and 'class' as\n",
    "# keys and labels, data['class'] as values.\n",
    "\n",
    "# Then make a 'ct' equal to pd.crosstab, taking the labels and class from\n",
    "# 'df'. Now, print it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's plot the inertia. \n",
    "\n",
    "# Use np.empty(8) to make an empty list, set it equal to inertia_list. \n",
    "\n",
    "# Use a for loop to instantiate 8 different KMeans clusters, each with 'i'\n",
    "# clusters. For each iteration make inertia_list[i] equal to the kmeans\n",
    "# inertia (there's an attribute for this). Print the result. \n",
    "\n",
    "\n",
    "# Let's plot:\n",
    "# plt.plot(range(0,8), inertia_list, '-o')\n",
    "# plt.xlabel('Clusters')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization. \n",
    "\n",
    "# Make a var 'data3' by dropping 'class'.\n",
    "\n",
    "# Instantiate scalar, kmeans (with two clusters), make a var 'pipe' with \n",
    "# scalar and kmeans passed into make_pipeline. Fit data3 with it. Derive \n",
    "# some predictions with it which we'll store in 'labels'. \n",
    "\n",
    "# Make the same 'df' as before and the same 'ct'. Print.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-SNE \n",
    "\n",
    "# Import TSNE from sklearn's 'manifold'. Instantiate w/ a learning rate \n",
    "# of 100 in var 'model'. Make a var 'transformed' equal to model's fit_transform\n",
    "# called on data2. \n",
    "\n",
    "# Use these by uncommenting them:\n",
    "# x = transformed[:, 0]\n",
    "# y = transformed[:, 1]\n",
    "\n",
    "# Now scatter plot x,y, and c (equal to color_list, from before)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "\n",
    "# Instantiate a var 'pca_mod'. Fit it on data3.\n",
    "\n",
    "# Make a var 'transformed' equal to the transformation of data3 with pca_mod.\n",
    "# Print the principle components so derived with the .components_ attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA, again. \n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# pca = PCA()\n",
    "# pipeline = make_pipeline(scaler, pca)\n",
    "# pipeline.fit(data3)\n",
    "\n",
    "# Now make a bar plot with this, whose x value is equal to the number of \n",
    "# components and whose y value is equal to the explained variance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
