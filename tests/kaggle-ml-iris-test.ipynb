{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "# First, the usual: pandas, numpy, matplotlib.pyplot and seaborn, with the standard aliases.\n",
    "\n",
    "# Second, our metrics and processing . We'll need a confusion matrix and a roc curve, \n",
    "# standard scaler, and train test split. \n",
    "\n",
    "# Third, for the models. We're doing logistic regression, knn, SVC, Gaussian naive bayes, \n",
    "# decision tree, and random forest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP ONE\n",
    "\n",
    "# Use this: \n",
    "# iris = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "\n",
    "# Make a df by reading the csv file at the url above, the separator will be a comma.\n",
    "\n",
    "\n",
    "# Set the df's columns to the following list of attributes:\n",
    "# attributes = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP TWO \n",
    "\n",
    "# We need an X and a y variable. Use .iloc to set X equal to the values from \n",
    "# all the rows from columns 0 to 4.\n",
    "# HINT: values are a df attribute, not a df method. \n",
    "\n",
    "# y needs to be equal to the values from the 'class' column \n",
    "# HINT: no .iloc needed here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP THREE\n",
    "\n",
    "# a) Do a standard train test split, 30% into the test vars, random state of 0\n",
    "# b) Instantiate a standard scaler object. Call it 'sc'. \n",
    "# c) Take the X_train var from a moment ago and set it equal to itself passed to sc's \n",
    "#    .fit_transform() method. \n",
    "# d) Do the same thing with X_test, but use sc's .transform() method instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP FOUR\n",
    "\n",
    "# We're going to make a logistic regression model. \n",
    "# a) Instantiate it, random state equal to 0\n",
    "# b) fit it on X_train and y_train \n",
    "# c) make a var log_reg_pred by passing in X_test to the model's .predict()\n",
    "# d) Print the model's score on X_test and y_test. \n",
    "\n",
    "# STEP FIVE \n",
    "\n",
    "# Now for the confusion matrix. \n",
    "# Make a var logreg_cm by passing y_test and the logreg's predictions into \n",
    "# the confusion matrix method. \n",
    "\n",
    "# STEP SIX\n",
    "\n",
    "# Plotting. Use the following, but try and think about what it's doing.\n",
    "# f, ax = plt.subplots(figsize = (5,5))\n",
    "# sns.heatmap(logreg_cm, annot=True, linewidth = .5, linecolor = 'red', fmt = \".0f\", ax=ax)\n",
    "# plt.xlabel('y_pred')\n",
    "# plt.ylabel('y_true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP SIX\n",
    "\n",
    "# We're going to do a k nearest neighbors classifier. \n",
    "# a) instantiate it (n_neighbors = 10) with a minkowski metric. \n",
    "# b) fit it on X_train and y_train. \n",
    "# c) make variable knn_pred with the model's prediction functionality. \n",
    "# d) Print out the model's accuracy score. \n",
    "\n",
    "# STEP SEVEN\n",
    "\n",
    "# Make another confusion matrix, give it an appropriate name.\n",
    "\n",
    "# STEP EIGHT\n",
    "\n",
    "# Use this: \n",
    "# f, ax = plt.subplots(figsize = (5,5))\n",
    "# sns.heatmap(knn_cm, annot=True, linewidths = .5, linecolor = 'red', fmt = '.0f', ax=ax)\n",
    "# plt.xlabel(\"knn_pred\")\n",
    "# plt.ylabel(\"y_true\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP NINE\n",
    "\n",
    "# Repeat the instantiation svc (kernel = 'poly'), fitting, and prediction \n",
    "# steps again with SVC.\n",
    "\n",
    "# STEP TEN \n",
    "\n",
    "# Make another appropriately-named confusion matrix. \n",
    "\n",
    "# STEP ELEVEN\n",
    "\n",
    "# Plot, again:\n",
    "\n",
    "# f, ax = plt.subplots(figsize = (5,5))\n",
    "# sns.heatmap(svc_cm, annot=True, linewidth = .5, linecolor = 'red', fmt = '.0f', ax=ax)\n",
    "# plt.xlabel(\"y_pred\")\n",
    "# plt.ylabel(\"y_true\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEPS TWELVE, THIRTEEN, FOURTEEN. \n",
    "\n",
    "# Do the above again, with Gaussian naive bayes. Nothing really changes \n",
    "# semantically or syntactically. \n",
    "\n",
    "# f, ax = plt.subplots(figsize = (5,5))\n",
    "# sns.heatmap(gnb_cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\n",
    "# plt.xlabel(\"y_pred\")\n",
    "# plt.ylabel(\"y_true\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEPS FIFTEEN, SIXTEEN, SEVENTEEN \n",
    "\n",
    "# From the top, with decision trees. In the instantiation phase we need \n",
    "# to specify a criterion parameter, for which we'll use entropy.\n",
    "\n",
    "# f, ax = plt.subplots(figsize =(5,5))\n",
    "# sns.heatmap(dtc_cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\n",
    "# plt.xlabel(\"y_pred\")\n",
    "# plt.ylabel(\"y_true\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEPS EIGHTEEN, NINETEEN, TWENTY \n",
    "\n",
    "# Repeat the above with a random forest. \n",
    "\n",
    "\n",
    "# f, ax = plt.subplots(figsize =(5,5))\n",
    "# sns.heatmap(rfc_cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\n",
    "# plt.xlabel(\"y_pred\")\n",
    "# plt.ylabel(\"y_true\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP TWENTY ONE  \n",
    "\n",
    "# Now we'll make a roc_curve. \n",
    "# a) Create a var y_prob with rfc's predict_proba method passed X_test. \n",
    "# b) With tuple unpacking populate variables fpr, tpr, and threshold. \n",
    "#    On the other side of the equals we'll pass three arguments into \n",
    "#    roc_curve: y_test, the row values from the zeroth column of y_proba\n",
    "#    and pos_label = 'e' (not sure what this is for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
